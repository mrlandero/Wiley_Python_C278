{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tony Landero   Date: 7/6/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_text(text):\n",
    "    text = text.lower()\n",
    "    return re.split('\\W+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'he',\n",
       " 'was',\n",
       " 'nearly',\n",
       " 'thirteen',\n",
       " 'my',\n",
       " 'brother',\n",
       " 'jem',\n",
       " 'got',\n",
       " 'his',\n",
       " 'arm',\n",
       " 'badly',\n",
       " 'broken',\n",
       " 'at',\n",
       " 'the',\n",
       " 'elbow',\n",
       " 'when',\n",
       " 'it',\n",
       " 'healed',\n",
       " 'and',\n",
       " 'jem창',\n",
       " 's',\n",
       " 'fears',\n",
       " 'of',\n",
       " 'never',\n",
       " 'being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'play',\n",
       " 'football',\n",
       " 'were',\n",
       " 'assuaged',\n",
       " 'he',\n",
       " 'was',\n",
       " 'seldom',\n",
       " 'self',\n",
       " 'conscious',\n",
       " 'about',\n",
       " 'his',\n",
       " 'injury',\n",
       " 'his',\n",
       " 'left',\n",
       " 'arm',\n",
       " 'was',\n",
       " 'somewhat',\n",
       " 'shorter',\n",
       " 'than',\n",
       " 'his',\n",
       " 'right',\n",
       " 'when',\n",
       " 'he',\n",
       " 'stood',\n",
       " 'or',\n",
       " 'walked',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'his',\n",
       " 'hand',\n",
       " 'was',\n",
       " 'at',\n",
       " 'right',\n",
       " 'angles',\n",
       " 'to',\n",
       " 'his',\n",
       " 'body',\n",
       " 'his',\n",
       " 'thumb',\n",
       " 'parallel',\n",
       " 'to',\n",
       " 'his',\n",
       " 'thigh',\n",
       " 'he',\n",
       " 'couldn창',\n",
       " 't',\n",
       " 'have',\n",
       " 'cared',\n",
       " 'less',\n",
       " 'so',\n",
       " 'long',\n",
       " 'as',\n",
       " 'he',\n",
       " 'could',\n",
       " 'pass',\n",
       " 'and',\n",
       " 'punt',\n",
       " 'when',\n",
       " 'enough',\n",
       " 'years',\n",
       " 'had',\n",
       " 'gone',\n",
       " 'by',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'us',\n",
       " 'to',\n",
       " 'look',\n",
       " 'back',\n",
       " 'on',\n",
       " 'them',\n",
       " 'we',\n",
       " 'sometimes',\n",
       " 'discussed',\n",
       " 'the',\n",
       " 'events',\n",
       " 'leading',\n",
       " 'to',\n",
       " 'his',\n",
       " 'accident',\n",
       " 'i',\n",
       " 'maintain',\n",
       " 'that',\n",
       " 'the',\n",
       " 'ewells',\n",
       " 'started',\n",
       " 'it',\n",
       " 'all',\n",
       " 'but',\n",
       " 'jem',\n",
       " 'who',\n",
       " 'was',\n",
       " 'four',\n",
       " 'years',\n",
       " 'my',\n",
       " 'senior',\n",
       " 'said',\n",
       " 'it',\n",
       " 'started',\n",
       " 'long',\n",
       " 'before',\n",
       " 'that',\n",
       " 'he',\n",
       " 'said',\n",
       " 'it',\n",
       " 'began',\n",
       " 'the',\n",
       " 'summer',\n",
       " 'dill',\n",
       " 'came',\n",
       " 'to',\n",
       " 'us',\n",
       " 'when',\n",
       " 'dill',\n",
       " 'first',\n",
       " 'gave',\n",
       " 'us',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'making',\n",
       " 'boo',\n",
       " 'radley',\n",
       " 'come',\n",
       " 'out',\n",
       " 'i',\n",
       " 'said',\n",
       " 'if',\n",
       " 'he',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'take',\n",
       " 'a',\n",
       " 'broad',\n",
       " 'view',\n",
       " 'of',\n",
       " 'the',\n",
       " 'thing',\n",
       " 'it',\n",
       " 'really',\n",
       " 'began',\n",
       " 'with',\n",
       " 'andrew',\n",
       " 'jackson',\n",
       " 'if',\n",
       " 'general',\n",
       " 'jackson',\n",
       " 'hadn창',\n",
       " 't',\n",
       " 'run',\n",
       " 'the',\n",
       " 'creeks',\n",
       " 'up',\n",
       " 'the',\n",
       " 'creek',\n",
       " 'simon',\n",
       " 'finch',\n",
       " 'would',\n",
       " 'never',\n",
       " 'have',\n",
       " 'paddled',\n",
       " 'up',\n",
       " 'the',\n",
       " 'alabama',\n",
       " 'and',\n",
       " 'where',\n",
       " 'would',\n",
       " 'we',\n",
       " 'be',\n",
       " 'if',\n",
       " 'he',\n",
       " 'hadn창',\n",
       " 't',\n",
       " 'we',\n",
       " 'were',\n",
       " 'far',\n",
       " 'too',\n",
       " 'old',\n",
       " 'to',\n",
       " 'settle',\n",
       " 'an',\n",
       " 'argument',\n",
       " 'with',\n",
       " 'a',\n",
       " 'fist',\n",
       " 'fight',\n",
       " 'so',\n",
       " 'we',\n",
       " 'consulted',\n",
       " 'atticus',\n",
       " 'our',\n",
       " 'father',\n",
       " 'said',\n",
       " 'we',\n",
       " 'were',\n",
       " 'both',\n",
       " 'right',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text(read_text_file(\"word_count.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tony\n",
      "[nltk_data]     Landero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Tony\n",
      "[nltk_data]     Landero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Tony\n",
      "[nltk_data]     Landero\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "def remove_stop_words(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in words if not w in stop_words and not w.isnumeric()]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize_words(words_clean):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in words_clean]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency_words(words_lemmatized):\n",
    "    freq = dict()\n",
    "    for w in words_lemmatized:\n",
    "        try:\n",
    "            freq[w] +=1\n",
    "        except:\n",
    "            freq[w] = 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_words_frequency(words_frequency, file_path=\"words_frequency.json\"):\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        json.dump(words_frequency,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_text_file(\"word_count.txt\")\n",
    "words = split_text(text)\n",
    "words_clean = remove_stop_words(words)\n",
    "words_lemmatized = lemmatize_words(words_clean)\n",
    "words_frequency = compute_frequency_words(words_lemmatized)\n",
    "save_words_frequency(words_frequency, file_path=\"words_frequency.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb00f0c01367cd29234ef98d6ee35d5dad54fff2cb1a8b567bc06dceecc7a7af"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
